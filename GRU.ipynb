{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3fab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdb16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\rohit\\Downloads\\archive\\Reddit_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd686b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'clean_comment' is NaN\n",
    "data = data.dropna(subset=['clean_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d59d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "max_words = 10000  # You can adjust this based on your dataset\n",
    "maxlen = 100  # Maximum length of a comment, you can adjust this based on your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ce2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['clean_comment'])\n",
    "sequences = tokenizer.texts_to_sequences(data['clean_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3313ba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54719 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6bc192",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pad = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847a4bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (37149, 100)\n",
      "Shape of label tensor: (37149,)\n"
     ]
    }
   ],
   "source": [
    "labels = np.asarray(data['category'])\n",
    "print('Shape of data tensor:', data_pad.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd85953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(data_pad, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8920132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 32)                12864     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,012,897\n",
      "Trainable params: 1,012,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Model Building\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Use 'sigmoid' for binary classification\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5a0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb969ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "929/929 [==============================] - 55s 57ms/step - loss: -0.6177 - acc: 0.5576 - val_loss: -1.7873 - val_acc: 0.6281\n",
      "Epoch 2/10\n",
      "929/929 [==============================] - 54s 58ms/step - loss: -4.2443 - acc: 0.6700 - val_loss: -5.3466 - val_acc: 0.6808\n",
      "Epoch 3/10\n",
      "929/929 [==============================] - 55s 59ms/step - loss: -8.6587 - acc: 0.6910 - val_loss: -8.7758 - val_acc: 0.6857\n",
      "Epoch 4/10\n",
      "929/929 [==============================] - 54s 58ms/step - loss: -13.4149 - acc: 0.6938 - val_loss: -12.5921 - val_acc: 0.7100\n",
      "Epoch 5/10\n",
      "929/929 [==============================] - 54s 58ms/step - loss: -18.0492 - acc: 0.6988 - val_loss: -16.4696 - val_acc: 0.6571\n",
      "Epoch 6/10\n",
      "929/929 [==============================] - 54s 58ms/step - loss: -23.4004 - acc: 0.7009 - val_loss: -19.9090 - val_acc: 0.7027\n",
      "Epoch 7/10\n",
      "929/929 [==============================] - 54s 58ms/step - loss: -28.5512 - acc: 0.7004 - val_loss: -24.4283 - val_acc: 0.6840\n",
      "Epoch 8/10\n",
      "929/929 [==============================] - 54s 58ms/step - loss: -33.8939 - acc: 0.7058 - val_loss: -28.5068 - val_acc: 0.6762\n",
      "Epoch 9/10\n",
      "929/929 [==============================] - 55s 59ms/step - loss: -39.0683 - acc: 0.7058 - val_loss: -31.3622 - val_acc: 0.6779\n",
      "Epoch 10/10\n",
      "929/929 [==============================] - 52s 56ms/step - loss: -45.0075 - acc: 0.7105 - val_loss: -34.4940 - val_acc: 0.6918\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bc55573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 3s 12ms/step - loss: -34.4940 - acc: 0.6918\n",
      "Test accuracy: 0.6917900443077087\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluation\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_val, y_val)\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
